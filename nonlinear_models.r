{
## Libraries used in the script ##
library(randomForest)
library(ranger)
library(xgboost)
library(mvtnorm)
library(caret)
library(e1071)
library(Metrics)
library(knitr)
library(tidyverse)
library(gbm)
library(RSNNS)

## Data generation ##

# For the purposes of this task, synthetic two-dimensional data (x1 and x2) were generated. 
# The number of samples has been set to 1000.

number_of_samples <- 1000
mean <- c(0,0)
sigma <- matrix(c(1, 0.6, 0.6, 1), nrow = 2)
set.seed(77)

non_linear_function <- as.data.frame(rmvnorm(n = number_of_samples, mean = mean, sigma = sigma))
colnames(non_linear_function) <- c("x1", "x2")


## Equation and noise ##

# The equation of the non-linear function was derived from the trigonometric functions of the 
# sum and difference of the angles. Then noise generated by rnorm() was added to the function.

non_linear_function <- non_linear_function %>%
  mutate(y_pure = cos(2*x1+0.5*x2) + sin(0.5*x2-8*x1)) %>%
  mutate(y_noise = y_pure + rnorm(number_of_samples, mean = 0, sd = 0.5))


## Visualisation of basic function ##

ggplot(non_linear_function, aes(x1, y_noise)) + 
  geom_line(aes(y = y_pure), color = "blue", alpha = 0.5) +
  geom_point(aes(y = y_noise), color = "red", alpha = 0.5) +
  labs(
    title = "Y values with noise vs pure Y values",
    x = "x1",
    y = "y",)


## Data division ##

# The next stage of the task was to divide the previously prepared data into two subsets: 
# training (75% of the base dataset) and test (25% of the base dataset).

split_data <- createDataPartition(non_linear_function$y_noise, p = 0.75, list = FALSE)
training_data <- non_linear_function[split_data,]
testing_data  <- non_linear_function[-split_data,]


## Preparation and tuning of chosen models ##

# Three models were selected to perform the task: Random Forest, XGBoost and SVM.

## Random Forest Model
rnd_forest <- randomForest(
  y_noise ~ ., 
  data = training_data, 
  ntree = 1000)

## Prediction for Random Forest
testing_data$y_rnd_forest_prediction <- predict(
  rnd_forest,
  testing_data)


## XGB Model
xgb <- xgboost(
  data = as.matrix(training_data[, c("x1", "x2")]),
  label = training_data$y_noise, 
  nrounds = 100)

## Prediction for XGB
testing_data$y_xgb_prediction <- predict(
  xgb, 
  as.matrix(testing_data[, c("x1", "x2")]))


## SVM Model
sv_machine <- svm(
  formula = y_noise ~ ., 
  data = training_data, 
  kernel = "radial", 
  cost = 1000)

## Prediction for SVM
testing_data$y_sv_machine_prediction <- predict(sv_machine, testing_data)


## Visualisation of final results ##

ggplot(testing_data, aes(x1, y_noise)) +
  geom_point(alpha = 0.5) +
  geom_line(aes(y = y_pure, color = "Original function"), size = 1) +
  geom_line(aes(y = y_rnd_forest_prediction, color = "Random Forest"), size = 1) +
  geom_line(aes(y = y_xgb_prediction, color = "XGBoost"), size = 1) +
  geom_line(aes(y = y_sv_machine_prediction, color = "SVM"), size = 1) +
  labs(
    title = "Models predictions comparison",
    x = "x1",
    y = "y",
    color = "Model types")


## Calculations ##

# In order to collect data on the basis of which it was possible to compare models and 
# decide which one is more accurate, the following calculations were made:
# - mean absolute errors
# - mean square errors
# Thanks to this, it was possible to later decide which models achieve the smallest prediction errors.


# Mean Absolute Error
mae_rf  <- mae(testing_data$y_noise, testing_data$y_rnd_forest_prediction)
mae_xgb <- mae(testing_data$y_noise, testing_data$y_xgb_prediction)
mae_svm <- mae(testing_data$y_noise, testing_data$y_sv_machine_prediction)
mae <- c(mae_rf, mae_xgb, mae_svm)

# Mean Square Error
mse_rf  <- mse(testing_data$y_noise, testing_data$y_rnd_forest_prediction)
mse_xgb <- mse(testing_data$y_noise, testing_data$y_xgb_prediction)
mse_svm <- mse(testing_data$y_noise, testing_data$y_sv_machine_prediction)
mse <- c(mse_rf, mse_xgb, mse_svm)


## Results and summary ##

types <- c("Random Forest", "XGBoost", "SVM")
performance_summary <- data.frame(types, mae, mse)
kable(performance_summary, caption = "Performance of differet types of models")

# Based on the table with the results containing the average errors, you can see that the XGBoost 
# and SVM models have similar values. The Random Forest model turned out to be the most effective.
}
